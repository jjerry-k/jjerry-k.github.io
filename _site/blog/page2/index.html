<!DOCTYPE html>
<html lang="en-us">
  <head>
  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; Jerry's Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="/public/css/main.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- scroll -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script>
    $( window ).scroll( function() {
      if ( $( this ).scrollTop() > 500 ) {
        $( '.top' ).fadeIn();
      } else {
        $( '.top' ).fadeOut();
      }
    } );
    $( '.top' ).click( function() {
      $( 'html, body' ).stop().animate( { scrollTop : 0 }, 100);
      return false;
    } );
  </script>
  
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (absbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-1054696837285307",
        enable_page_level_ads: true
    });
  </script>
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      //jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
  
  
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <script src="/dest/simple-jekyll-search.js" type="text/javascript"></script>
  <script type="text/javascript">
  $(document).ready(function(){
    document.search.searchinput.focus();
  });
  </script>
</head>

  <style>blockquote {font-size: 1em; line-height: 1.4}</style>
  </head>
  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="https://gravatar.com/22a6447c0c965de55f4ce9691aed2af4">
          <img src="https://www.gravatar.com/avatar/22a6447c0c965de55f4ce9691aed2af4?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p></p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me  :  
        
        
        
        <a href="https://github.com/jjerry-k">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="https://www.facebook.com/jerry.kim.566">
          <i class="fa fa-facebook" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:jaeyeol2931@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Contents
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/blog/">
                Blog
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/python/">
                Python
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/ubuntu/">
                Ubuntu
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/living/">
                Living
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/deeplearning/">
                Deep Learning
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 Jerry. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
          <div class="top" style="position:fixed; right:10%; bottom:15px; display:none; z-index:9999;">
            <a href="#">
              <img src="https://jjerry-k.github.io/public/top.png" width="40" height="40" class="top" style="border-radius:5px; background-color: #000; margin-bottom:0; margin-right:7px; float:left;">
            </a>
            
            <a href="/blog//">
              <img src="https://jjerry-k.github.io/public/contents.png" width="40" height="40" class="top" style="border-radius:5px; background-color: #000; margin-bottom:0; float:left;">
            </a>
          </div>
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title" align="center">
            <a href="/" title="Home" title="Jerry's Blog">
              <img class="masthead-logo" src="/public/logo.png"/>
            </a>
            <small></small>
            <img src="http://jjerry-k.github.io/public/search.png" width="20" height="20" style="position:absolute; top:1.3rem; right:1.5rem" data-toggle="modal" data-target="#myModal">
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/02/16/Xception/">
        Xception
      </a>
    </h1>

    <span class="post-date">16 Feb 2020</span>
     | 
    
    <a href="/blog/tags/#paper" class="post-tag">Paper</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <h1 id="xception-deep-learning-with-depthwise-separable-convolution">Xception: Deep Learning with Depthwise Separable Convolution</h1>

<p>Author: Franc¸ois Chollet<br />
Date: Dec 19, 2016<br />
URL: https://arxiv.org/abs/1610.02357</p>

<hr />

<h1 id="abstract">Abstract</h1>

<ul>
  <li>CNN 에서 많이 사용되는 Inception 은 Regular Convolution 과 Depthwise Separable Convolution 의 중간 단계.</li>
  <li>Depthwise Separable Convolution 은 최대 개수의 타워를 가진 Inception module (?)</li>
  <li>Inception module을 Depthwise Separable Convolution 으로 대체한 새로운 Deep Convolutional Neural Network 제안.</li>
  <li>Xception 이라고 칭함.</li>
  <li>ImageNet 으로 학습된 Inception V3 보다 살짝 성능이 좋고 350,000,000개의 이미지와 17,000개의 클래스로 구성된 Larger image dataset 에선 Inception V3 보다 월등히 뛰어난 성능을 보임.</li>
  <li>Inception V3와 Xception 이 동일한 parameter 수를 가짐.</li>
  <li>그러므로 연산량, 메모리의 증가가 아닌 모델 parameter를 효과적으로 사용해서 성능 향상이 된 것.</li>
</ul>

<h1 id="1introduction"><strong>1. Introduction</strong></h1>

<ul>
  <li>Convolutional Neural Network (이하 CNN) 은 Computer Vision 에서 가장 주요한 알고리즘이 되었고, 이를 설계하는 방법에 대해 개발하는데 많은 관심을 가지게 됨.</li>
  <li>Lenet -&gt; AlexNet (2012) -&gt; ZFNet (2013) -&gt; VGG (2014) -&gt; Inception 종류 … -&gt; Inception-ResNet (2015)</li>
  <li>Inception 스타일의 기본 구성 요소는 Inception module.</li>
  <li>
    <p>Figure 1은 Inception V3 의 표준 Inception Module.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled.png" /></p>
  </li>
  <li>Inception 모델은 위와 같은 모듈을 Stack 한 것. VGG-Style 네트워크는 단순히 Convolution layer를 Stack.</li>
  <li>실험적으로 Inception-style 이 VGG-style보다 적은 parameter로 다양한, 많은 feature를 학습 할 수 있다는 것을 보임.</li>
</ul>

<h2 id="11theinceptionhypothesis"><strong>1.1 The Inception hypothesis</strong></h2>

<ul>
  <li>Convolution layer는 3차원 공간에서 filter를 학습하려고 함.</li>
  <li>Single Convolution kernel 은 채널의 correlation과 공간의 correlation 을 동시에 mapping 함.</li>
  <li>Inception 모듈은 채널, 공간의 correlation 을 독립적으로 나타낼 수 있도록 연산을 분해. -&gt; 쉽고 효율적인 프로세스를 만듦.</li>
  <li>Inception의 가설은 채널 채널, 공간의 correlation 이 분리되어 있으므로 동시에 매핑하는 것은 좋지 않다는 것.</li>
  <li>
    <p>Figure 2는 3x3 conv와 1x1 conv만 사용한 단순화한 Inception 모듈.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_1.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_1.png" /></p>
  </li>
  <li>
    <p>Figure 3은 Figure 2의 Inception 모듈에서 하나의 큰 1x1 Convolution과 3x3 Convolution들로 재구성한 것.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_2.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_2.png" /></p>
  </li>
  <li>이 방법이 Inception 의 가설보다 뛰어난 가설을 만드는 것이 합리적인 것일지, 채널과 공간을 독립적으로 매핑할 수 있는지 의문.</li>
</ul>

<h2 id="12thecontinuumbetweenconvolutionsandseparableconvolutions"><strong>1.2 The continuum between convolutions and separable convolutions</strong></h2>

<ul>
  <li>
    <p>Figure 4 처럼 Inception 모듈 구성.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_3.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_3.png" /></p>
  </li>
  <li>1x1 Convolution 적용하여 채널의 correlation 매핑, 그 후 각각의 channel별로 공간의 correlation 매핑</li>
  <li>이를 <strong>An “extreme” version of an Inception module</strong> 이라고 칭함.</li>
  <li>TensorFlow 프레임워크에 Depthwise Separable Convolution 연산과 거의 동일함.</li>
  <li>TensorFlow나 Keras 프레임워크에 있는 Depthwise Separable Convolution (Separable Convolution 라고도 불림.) 은 각 channel 별로 3x3 Convolution 적용 후 채널간의 1x1 Convolution 적용.</li>
  <li>영상처리 분야에서 사용하는 Separable Convolution 과 혼동하면 안됨, 이 연산은 공간적 분리를 하는 Convolution.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">비교</th>
      <th style="text-align: center">Extream</th>
      <th style="text-align: center">Depthwise Separable</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">연산순서</td>
      <td style="text-align: center">pointwise–&gt;channelwise</td>
      <td style="text-align: center">channelwise–&gt;pointwise</td>
    </tr>
    <tr>
      <td style="text-align: center">비선형성</td>
      <td style="text-align: center">Presence</td>
      <td style="text-align: center">Absence</td>
    </tr>
  </tbody>
</table>

<h1 id="2priorwork"><strong>2. Prior work</strong></h1>

<ul>
  <li>VGG-16 과 같은 구조가 xception 과 유사.</li>
  <li>Inception 구조는 가지치기의 이점을 보여줌.</li>
  <li>Depthwise separable convolution는 경량화에도 적합.</li>
  <li>TensorFlow에는 이미 구현되어있음.</li>
  <li>Residual connection 을 광범위하게 사용.</li>
</ul>

<h1 id="3thexceptionarchitecture"><strong>3. The Xception architecture</strong></h1>

<p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_4.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_4.png" /></p>

<ul>
  <li>Figure 5 와 같은 구조 제안.</li>
  <li>처음과 마지막을 제외하곤 linear residual module 사용.</li>
  <li>총 36개의 convolution layer로 구성.</li>
  <li>매우 단순한 구조.</li>
</ul>

<h1 id="4experimentalevaluation"><strong>4. Experimental evaluation</strong></h1>

<ul>
  <li>Xception과 Inceotion V3 비교.</li>
  <li>Parameters가 비슷. 네트워크 규모에 대한 차이를 없애기 위함.</li>
  <li>ImageNet과 JFT dataset 이용.</li>
</ul>

<h2 id="41thejftdataset"><strong>4.1 The JFT dataset</strong></h2>

<ul>
  <li>그냥…JFT 데이터 설명…</li>
  <li>Google 데이터 중 하나</li>
  <li>JFT 로 학습, FastEval14k dataset으로 성능 비교.</li>
</ul>

<h2 id="42optimizationconfiguration"><strong>4.2 Optimization configuration</strong></h2>

<ul>
  <li>각 방법에 대해서 다음과 같은 설정으로 Xception, Inception V3 모두 학습</li>
  <li>On ImageNet
    <ul>
      <li>Optimizer: SGD</li>
      <li>Momentum: 0.9</li>
      <li>Initial learning rate: 0.045</li>
      <li>Learning rate decay: 0.94 (every 2 epochs)</li>
    </ul>
  </li>
  <li>On JFT
    <ul>
      <li>Optimizer: RMSprop</li>
      <li>Momentum: 0.9</li>
      <li>Initial learning rate: 0.001</li>
      <li>Learning rate decay: 0.9 (every 3,000,000 samples)</li>
    </ul>
  </li>
</ul>

<h2 id="43regularizationconfiguration"><strong>4.3 Regularization configuration</strong></h2>

<ul>
  <li>Weight decay</li>
  <li>Dropout</li>
  <li>Auxiliary loss tower</li>
</ul>

<h2 id="44traininginfrastructure"><strong>4.4 Training infrastructure</strong></h2>

<ul>
  <li>80개의 NVIDIA K80 GPU ….</li>
  <li>ImageNet 학습시 <strong>synchronous gradient descent</strong>을 적용하여 data parallelism 이용. → 3일 소요</li>
  <li>JFT 학습시 <strong>asynchronous gradient descent</strong>을 적용하여 data parallelism 이용. →한달 소요</li>
</ul>

<h2 id="45comparisonwithinceptionv3"><strong>4.5 Comparison with Inception V3</strong></h2>

<h3 id="451classificationperformance"><strong>4.5.1 Classification performance</strong></h3>

<ul>
  <li>
    <p>두 데이터 모두 Xception이 좋은 성능을 보임.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_5.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_5.png" /></p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_6.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_6.png" /></p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_7.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_7.png" /></p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_8.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_8.png" /></p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_9.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_9.png" /></p>
  </li>
</ul>

<h3 id="452sizeandspeed"><strong>4.5.2 Size and speed</strong></h3>

<ul>
  <li>
    <p>Parameter가 늘지 않으면서 성능 향상을 보이기에 Xception이 효율적인 모델.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_10.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_10.png" /></p>
  </li>
</ul>

<h3 id="46effectoftheresidualconnections"><strong>4.6 Effect of the residual connections</strong></h3>

<ul>
  <li>Residual connection에 대한 ablation study 진행.</li>
  <li>Residual connection의 중요성을 보여줌.</li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_11.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_11.png" /></p>

<h3 id="47effectofanintermediateactivationafterpointwiseconvolutions"><strong>4.7 Effect of an intermediate activation after point wise convolutions</strong></h3>

<ul>
  <li>Depthwise separable convolution은 depthwise → pointwise convolution으로 구성되어있음.</li>
  <li>
    <p>그 중간에 activation function에 대한 ablation study 진행.</p>

    <p><img src="https://jjerry-k.github.io/public/img/xception/Untitled_12.png" alt="https://jjerry-k.github.io/public/img/xception/Untitled_12.png" /></p>
  </li>
  <li><a href="https://arxiv.org/pdf/1512.00567.pdf">Inception module에 대한 연구</a>와 반대되는 결과 도출.</li>
</ul>

<h1 id="5futuredirections"><strong>5. Future directions</strong></h1>

<ul>
  <li>Depthwise separble convolution 이 만능이라는 보장은 없음.</li>
</ul>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/02/16/Xception/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/02/16/Xception/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/python/2019/09/07/Jupyter_shortcut/">
        jupyter Lab 에서 단축키 설정하는 방법!
      </a>
    </h1>

    <span class="post-date">07 Sep 2019</span>
     | 
    
    <a href="/blog/tags/#setting" class="post-tag">Setting</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p>파이썬, 딥러닝을 하시는 분들 중 Jupyter 부류를 이용하시는 분들이 꽤 많습니다. <code class="highlighter-rouge">Jupyter Notebook, Jupyter Lab,...</code><br />
저는 둘 다 사용해보긴 하지만 주로 <strong>Jupyter Lab</strong> 을 사용합니다.<br />
이유는 그냥…좀 더 보기 편해서..?<br />
두 환경의 차이는 다음 링크에서 확인해주세요.</p>
<ul>
  <li><a href="https://jupyter.readthedocs.io/en/latest/running.html#running">Jupyter Notebook</a></li>
  <li><a href="https://jupyterlab.readthedocs.io/en/stable/index.html">Jupyter Lab</a></li>
</ul>

<h4 id="참고사항">참고사항</h4>
<p>https://github.com/jupyterlab/jupyterlab/issues/7122<br />
며칠 전 이슈에 올라온 상황입니다.<br />
jupyter lab 1.0.2 버전에 extension을 설치하면 sidebar 부분이 깨지는 버그가 있네요…<br />
1.1.1 에선 고쳐졌다고 합니다.</p>

<p><strong><span style="font-size:25pt;">자, 다시 본론으로 돌아가봅니다.</span></strong></p>

<p>Jupyter Lab 에서 단축키로 설정할 수 있는 기능들은 다음과 같습니다.</p>

<p>‘notebook:create-new’<br />
‘notebook:interrupt-kernel’<br />
‘notebook:restart-kernel’<br />
‘notebook:restart-clear-output’<br />
‘notebook:restart-run-all’<br />
‘notebook:reconnect-to-kernel’<br />
‘notebook:change-kernel’<br />
… (너무 많다….)<br />
‘notebook:hide-all-cell-outputs’<br />
‘notebook:show-all-cell-outputs’<br />
‘notebook:enable-output-scrolling’<br />
‘notebook:disable-output-scrolling’<br />
‘notebook:save-with-view’</p>

<p>너무 많은 관계로 <strong><a href="https://github.com/jupyterlab/jupyterlab/blob/af548c2674427da79d54ad5c4b69bb175463e9a0/packages/notebook-extension/src/index.ts#L69-L197">링크</a></strong> 참고해주세요.</p>

<p>이번 포스팅에선 예시로 
<code class="highlighter-rouge">Restart and Clear</code> 에 대해서 shortcut을 설정해보겠습니다.</p>

<h2 id="설정-방법">설정 방법</h2>

<p>먼저 Jupyter Lab 을 실행시켜주세요.</p>

<p><img src="https://jjerry-k.github.io/public/img/shortcut/fig01.png" alt="fig1" /></p>

<p><strong>Advanced Setting Editor</strong> 를 클릭해주세요.</p>

<p><img src="https://jjerry-k.github.io/public/img/shortcut/fig02.png" alt="fig2" /></p>

<p>좌측에 <strong>Keyboard Shortuts</strong> 를 선택해주세요.</p>

<p><img src="https://jjerry-k.github.io/public/img/shortcut/fig03.png" alt="fig3" /></p>

<p>여기서 왼쪽의 창은 default setting에 대한 설명이고 오른쪽을 Custom 을 위해 기입하는 부분입니다.</p>

<p>그럼 예시대로 <code class="highlighter-rouge">Restart and Clear</code> 에 대해서 shortcut을 설정해보겠습니다.</p>

<p>위에 기능에 대해서 링크 올려드렸었죠? 그 깃헙에서 링크에 대한 내용을 먼저 찾아봅니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/shortcut/fig04.png" alt="fig4" /></p>

<p>거기서 ‘{기능 이름}’ 부분을 복사해주세요.</p>

<p>그리고 다음과 같이 기입해줍니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/shortcut/fig05.png" alt="fig5" /></p>

<p>그리고 우측 상단에 저장버튼을 누르고 노트북에서 Shift + Command + C 를 눌렀을때 <code class="highlighter-rouge">Restart and Clear</code> 이 작동하게 됩니다.</p>

<p>사진만 보고 따라 적으시기 귀찮으실테니 code block 으로 남깁니다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"shortcuts": [
    {
        "command": "kernelmenu:restart-and-clear",
        "keys": ["Shift + Ctrl + C"],
        "selector": "[data-jp-kernel-user]:focus"
        }]
</code></pre></div></div>

<p>과정을 요약해드리면 (Jupyter Lab 켜기 ~ Editor 열기 까지는 생략)</p>
<ol>
  <li>기능에 대한 링크에서 자신이 원하는 기능 찾기</li>
  <li>위에 code block 에서 “command”: 부분에 ‘{기능 이름}’ 기입</li>
  <li>“keys”: 에 원하는 커맨드 넣기 (복수개 가능)</li>
  <li>“selector”: 에 .jp-Notebook:focus” 혹은 “[data-jp-kernel-user]:focus” 기입</li>
  <li>저장 후 즐겁게 사용!</li>
</ol>

<p>shortcut 설정은 여기까지 입니다.<br />
혹시 이외에 다른 설정에 대해 궁금한거 있으시면 comment 남겨주세요!</p>

<h3 id="참고사항-1">참고사항</h3>
<blockquote>
  <p>만약 <strong>A 기능은 Shift + Alt + C</strong> 이고 <strong>B 기능은 Alt + Shift + C</strong> 일때… 전체적으로 보면 같은 키 입력이지만 순서가 다르기 때문에 <strong>별개의 shortcut으로 작동</strong>합니다.</p>
</blockquote>

    <article></h4>
    <div class="post-more">
      
      <a href="/python/2019/09/07/Jupyter_shortcut/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/python/2019/09/07/Jupyter_shortcut/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/python/2019/07/10/Multiple_Kernel/">
        jupyter (ipython) 여러 커널 사용하기!
      </a>
    </h1>

    <span class="post-date">10 Jul 2019</span>
     | 
    
    <a href="/blog/tags/#setting" class="post-tag">Setting</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p>Anaconda 를 사용하다보면 여러 가상환경을 만들게 됩니다. (아닐수도 있구요…)<br />
그 후에 jupyter 를 사용하시는 분들이라면 대부분 이렇게 사용하실 겁니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate 환경이름
jupyter notebook
</code></pre></div></div>

<p><code class="highlighter-rouge">conda activate 환경이름</code> 이라는걸 <strong>무.조.건</strong> 써줘야하죠..<br />
이게 매우 귀찮았습니다…</p>
<blockquote>
  <p>“activate 없이 base에서 <code class="highlighter-rouge">jupyter notebook</code>을 실행해도 가상환경을 잡을 수 있는 방법이 없나..”</p>
</blockquote>

<p>이런 생각 많이들 하실 것 같습니다.<br />
당연히 방법이 있습니다!<br />
그 방법에 대해 알려드리겠습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate <span class="o">{</span>환경이름<span class="o">}</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--user</span> <span class="nt">--name</span> <span class="o">{</span>환경이름<span class="o">}</span> <span class="nt">--display-name</span> <span class="o">{</span>Jupyter에 표시될 이름<span class="o">}</span>
</code></pre></div></div>

<p>이렇게 하시면 됩니다. 예시를 직접 보여드리겠습니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/multi_kernel/01.png" /></p>

<p>저는 base 환경만 썼습니다. 가상환경을 가볍게 하나 만들도록 할게요!</p>

<p><img src="https://jjerry-k.github.io/public/img/multi_kernel/02.png" /></p>

<p><code class="highlighter-rouge">test</code>라는 가상환경을 만들었습니다. 먼저 <strong>kernel을 추가하지 않고</strong> jupyter notebook을 실행해보겠습니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/multi_kernel/03.png" />
<img src="https://jjerry-k.github.io/public/img/multi_kernel/04.png" /></p>

<p>첫번째 사진을 보시면 <strong>Python 3</strong> 만 나오는걸 보실 수 있습니다. 저 <strong>Python 3</strong> 는 base의 Python을 가리킵니다.<br />
base에는 제가 tensorflow를 설치해놨기 때문에 import 가 잘 작동하는군요..<br />
그럼 kernel을 추가해보겠습니다.  (일단 test 환경에 ipython 이 안깔려있어서 설치함..)</p>

<p><img src="https://jjerry-k.github.io/public/img/multi_kernel/05.png" />
<img src="https://jjerry-k.github.io/public/img/multi_kernel/06.png" /></p>

<p>커널을 추가하고 나면 <code class="highlighter-rouge">Installed ~~~~</code> 라는 메세지를 보실 수 있어요!<br />
그럼 다시 base로 돌아가서 jupyter notebook을 실행해보겠습니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/multi_kernel/07.png" />
<img src="https://jjerry-k.github.io/public/img/multi_kernel/08.png" /></p>

<p>첫번째 사진을 보시면 추가하기 전과는 다르게 <strong>test</strong> 라는 항목이 추가되었습니다!<br />
실행을 해봐도 tensorflow 모듈이 없다고 나오는걸 확인하실 수 있습니다!<br />
앞으로는 <code class="highlighter-rouge">conda activate ~~</code>를 안하셔도 되요! (뿌듯)</p>

<p>이번 포스팅은 여기까지 입니다.<br />
많은 분들께 도움이 되었으면 좋겠네요!</p>

<p><strong>PS. jupyter notebook 우측 상단쪽에 어떤 커널로 실행하고 있는지 표시가 됩니다.  (<code class="highlighter-rouge">Logout</code> 아래)</strong></p>

    <article></h4>
    <div class="post-more">
      
      <a href="/python/2019/07/10/Multiple_Kernel/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/python/2019/07/10/Multiple_Kernel/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/python/2019/07/06/DICOM/">
        Python으로 DICOM 영상을 읽어보자!
      </a>
    </h1>

    <span class="post-date">06 Jul 2019</span>
     | 
    
    <a href="/blog/tags/#usage" class="post-tag">Usage</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p>예~~전에 <a href="https://jjerry-k.github.io/python/2019/01/23/nifti/">NIfTI</a> 파일을 load 하는 방법을 올렸었습니다! <br />
이번에는 Python에서 <a href="https://www.dicomstandard.org">DICOM</a> 포맷의 데이터를 load 하는 방법에 대한 포스팅을 해보려고 합니다.</p>

<p>가장 먼저 관련 패키지인 <a href="https://pydicom.github.io/pydicom/stable/index.html"><code class="highlighter-rouge">Pydicom</code></a> 을 설치를 해줍니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge pydicom
</code></pre></div></div>

<p>저번과 똑같이 단순한 설치방법!</p>

<p>이제 코딩으로 읽어보겠습니다.<br />
예시 데이터로 다음 <a href="https://github.com/pydicom/pydicom/blob/master/pydicom/data/test_files/MR_small.dcm">링크</a>에 있는 영상을 이용하였습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">pydicom</span> <span class="k">as</span> <span class="n">di</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">di</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="s">".dcm 경로"</span><span class="p">)</span>
<span class="c1">#data = di.dcmread(".dcm 경로") # 편한거 쓰시면 됩니다.
</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pixel_array</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="c1"># 슬라이스 1장일 경우
#plt.imshow(img[:,:,"slice 번호"]) # 슬라이스가 여러 장일 경우
</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>이런 식으로 작성하시면 됩니다.<br />
 예시를 보여드리면</p>

<p><img src="https://jjerry-k.github.io/public/img/dicom/01.png" /></p>

<p>예시 영상이 좀 작네요…<br />
어쨌든 이런 식으로 읽습니다.</p>

<p>흠….NIfTI와 DICOM을 했으니…다음엔 <strong>Insight Meta-Image</strong> 를 해보겠습니다!</p>

    <article></h4>
    <div class="post-more">
      
      <a href="/python/2019/07/06/DICOM/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/python/2019/07/06/DICOM/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2019/07/05/Thin/">
        MRI interpolation using Deep Learning - [1]
      </a>
    </h1>

    <span class="post-date">05 Jul 2019</span>
     | 
    
    <a href="/blog/tags/#paper" class="post-tag">Paper</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <h1 id="deep-generative-adversarial-networks-for-thin-section-infant-mr-image-reconstruction">Deep Generative Adversarial Networks for Thin-Section Infant MR Image Reconstruction</h1>

<ul>
  <li>Jiaqi Gu<sup>1</sup>, Zezu Li<sup>1</sup>, YuanYuan Wans<sup>1, 3</sup>, Haowei Yang<sup>2</sup>, Zhongwei Qiao<sup>2</sup>, and Jinhua Yu<sup>1, 3</sup></li>
  <li><sup>1</sup>School of Information Science and Technology, Fudan University, Shanghai 200433, China<br />
<sup>2</sup>The Children’s Hospital of Fudan University, Shanghai 201102, China<br />
<sup>3</sup>Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention of Shanghai, Department of Electronic Engineering, Institute of Functional
and Molecular Medical Imaging, Fudan University, Shanghai 200433, China
—</li>
</ul>

<h2 id="abstract">Abstract</h2>
<ul>
  <li>Thin section magnetic resonance images (<strong>Thin MRI</strong>) 는 뇌수술, 뇌 구조 분석에 좋은 영상.</li>
  <li>하지만 Thick section magnetic resonance images (<strong>Thick MRI</strong>) 에 비해 imaging cost가 많이 들기 때문에 잘 사용되지 않음.</li>
  <li>Thick MRI 2 Thin MRI 제안.</li>
  <li>Two stage( GAN -&gt; CNN )로 구성하였고 Thick MRI의 Axial, Sigittal plane을 이용하여 Thin MRI의 Axial reconstruction.</li>
  <li>3D-Y-Net-GAN 은 Axial, Sagittal Thick MRI 를 이용하여 Fusion.</li>
  <li>3D-Dense U-Net은 Sagittal plane에 대해 세부적인 calibrations, structual correction 제공.</li>
  <li>Loss function 은 structual detail을 Network가 capture 할 수 있도록 제안.</li>
  <li>bicubic, sparse representation, 3D-SRU-Net 과 비교.</li>
  <li>35번의 Cross-validation, 114개를 이용하여 두개의 testset 구성.
    <ul>
      <li>PSNR : 23.5 % 증가.</li>
      <li>SSIM : 90.5 % 증가.</li>
      <li>MMI : 21.5 % 증가.</li>
    </ul>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>
<ul>
  <li>Thin MRI 는 slice thickness가 1mm이고 sapcing gap이 0mm.</li>
  <li>하지만 항상 Thin MRI를 사용할 수 없음.</li>
  <li>일반적으로 사용하는 Thick MRI는 slice thickness가 4~6mm 이고 sapcing gap이 0.4~1mm.
    <ul>
      <li>해상도 : Thin MRI &gt; Thick MRI</li>
    </ul>
  </li>
  <li>인간의 뇌 발달에 대한 insight를 주기 때문에 유아의 brain MR image는 어른의 brain MR image 보다 연구에 가치가 있음</li>
  <li>하지만 유아의 MR image를 얻는게 쉽지 않음.</li>
  <li>그래서 Thick to Thin 제안.</li>
  <li>기존 traditional interpolation algorithm
    <ul>
      <li>시각적으로는 성능이 좋아보임. 하지만 성인의 brain 에 초점을 맞춤.</li>
      <li><strong><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4478865/pdf/JMI-001-034007.pdf">Interpolation-based super-resolution reconstruction: effects of slice thickness</a></strong></li>
      <li><strong><a href="https://www.hindawi.com/journals/ijbi/2013/395915/">Evaluation of interpolation effects on upsampling and accuracy of cost functions-based optimized automatic image registration</a></strong></li>
    </ul>
  </li>
  <li>Frame interpolation 방법과 같이 적용 가능.
    <ul>
      <li><strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8110374">Slice Interpolation in MRI Using a Decomposition-reconstruction Method</a></strong></li>
    </ul>
  </li>
  <li>Super-resolution 문제로 적용할 수도 있음.
    <ul>
      <li><strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5466111">Image super-resolution via sparse representation</a></strong></li>
    </ul>
  </li>
  <li>CNN, GAN 이 발전하면서 super-resolution 이 탄력을 받음.
    <ul>
      <li><strong>[<a href="https://link.springer.com/chapter/10.1007/978-3-319-67564-0_12">Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging</a></strong>]</li>
      <li><strong>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417964">Deep Generative Adversarial Neural Networks for Compressive Sensing MRI</a>]</strong></li>
    </ul>
  </li>
  <li>이전에 성인의 Thick MRI를 Thin MRI 로 reconstruction 하는 3D-SRGAN 제안했으나 axial plane만 고려했음. <strong>[<a href="https://link.springer.com/chapter/10.1007%2F978-3-319-67389-9_38">Reconstruction of Thin-Slice Medical Images Using Generative Adversarial Network</a>]</strong></li>
  <li>Deep Learning 이 reconstruction performance 뿐 아니라 reconstruction time 감소에도 매우 효과적인걸 보임.</li>
</ul>

<h2 id="proposed-method">Proposed Method</h2>

<h3 id="a-overview">A. Overview</h3>
<ul>
  <li>CNN은 기존에도 super-resolution에서 많이 사용됨.</li>
  <li>하지만 최근까지 제안된 Network는 대부분 2D image에 대한 upscaling.</li>
  <li>몇몇 Network는 3D image로 확장했지만 그렇게 효과를 보지 못했음.</li>
  <li>이 논문의 Flow
<img src="https://jjerry-k.github.io/public/img/thin/fig01.PNG" /></li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/thin/fig02.PNG" /></p>

<h3 id="b-network-architecture">B. Network Architecture</h3>
<ul>
  <li>First stage는 3D-Y-Net-GAN 으로 Thick MRI를 Thin MRI로 생성 후 3D-DenseU-Net으로 recalibration.</li>
</ul>

<h4 id="3d-y-net-gan">3D-Y-Net-GAN</h4>
<ul>
  <li>Input : Axial, Sagittal Thick MRI</li>
  <li>Output : Thin MRI</li>
  <li>r : Upscaling Factor ( r = 8 일 경우의 예시 )</li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/thin/fig03.PNG" /></p>

<ul>
  <li>Feature Extraction Branches
    <ul>
      <li>각 input에 대한 feature 추출.</li>
      <li>Maxpooling layer에서 [1, 2, 1], [2, 1, 1]의 다른 strides factor 적용.</li>
      <li>3D convolutional layer 는 Convolution + Batch Normalization + Swish 로 구성.
        <ul>
          <li><a href="https://arxiv.org/pdf/1710.05941.pdf">Swish</a>는 Activation 의 종류로 ReLU로 인해 생기는 Dead neuron을 극복할 수 있음. <strong>-&gt; 근데 굳이 왜 swish일까…</strong></li>
        </ul>
      </li>
      <li>layers 를 거친 후 shape 의 변화
        <ul>
          <li>Axial : [H, W, S, 1] -&gt; [H/2, W/2, S, 32]</li>
          <li>Sagittal : [H, W, r*S, 1] -&gt; [H/2, W/2, S, 32]</li>
        </ul>
      </li>
      <li>Axial과 Sagittal의 shape이 다르기 때문에 Sagittal 에 대해서 preprocessing으로 3개의 3d convolution layer 적용.</li>
    </ul>
  </li>
  <li>Feature Fusion Branch
    <ul>
      <li>두 feature를 channel 방향으로 Concatanation.</li>
      <li>W 방향으로 Upsampling 후 H 방향으로 Downsampling feature 를 Concatanation.</li>
      <li>H 방향으로 Upsampling 후 첫번째 Block의 Feature map을 Concatanation</li>
      <li>U-Net 에서 아이디어를 얻었고 structual alignment, gradient-vanishing 등을 완화.</li>
    </ul>
  </li>
  <li>Reconstruction Branch
    <ul>
      <li>Figure 3 (b) 와 같은 구조.</li>
      <li>Upsampling layer 3개를 연속으로 붙여서 8배 확장하는 구조 대신에 Multipath upscaling strategy 적용. <strong>-&gt; Artifact 완화 효과…?</strong></li>
    </ul>
  </li>
  <li>
    <p>Discriminator
<img src="https://jjerry-k.github.io/public/img/thin/fig04.PNG" /></p>

    <ul>
      <li>Axial Image, Saggital Image, Combination Image 가 Real Pair인지 Fake Pair인지 감별.</li>
      <li>Input : ($I^A$, $I^Y$, $I^S$), ($I^A$, $I^{GT}$, $I^S$)</li>
      <li>Output : Real, Fake</li>
    </ul>
  </li>
</ul>

<h4 id="3d-denseu-net">3D-DenseU-Net</h4>
<p><img src="https://jjerry-k.github.io/public/img/thin/fig05.PNG" /></p>

<ul>
  <li>전체적인 구조는 U-Net이지만 2개의 Enhanced residual block 을 적용하여 detail recalibration.</li>
  <li>Input :  $I^Y$, $I^S$, $I^{YA}$ <strong>-&gt; 어떻게 3개가 input으로…?</strong></li>
  <li>Output : Thin MR Image</li>
  <li>$I^A$ 를 $I^Y$ 의 해당 위치에 insertion 하여 $I^{YA}$ 생성. -&gt; 아직 이해 X..
    <ul>
      <li>Axial Information 을 이용하여 정확한 axial 을 만들기 위해…</li>
      <li>$I^S$ 를 $I^Y$ 에 insertion하게 되면 Sagittal 에 대한 information 이 과해지기 때문에 Reconstrtion Axial Image의 Quality 가 안좋아 질 것!</li>
    </ul>
  </li>
  <li>End-to-End 가 아니라 각각 따로따로 학습. <strong>-&gt; Faster RCNN 과 같은 방식으로 할런지….?</strong></li>
</ul>

<h4 id="loss-function">Loss Function</h4>
<ul>
  <li>$G$ 는 generator 라는 의미.</li>
  <li>Self-Adaptive Charbonnier Loss
    <ul>
      <li>일반적으로 많이 사용되는 $\ell2$ 전반적으로 Smoothing 하게 만들어지고 $\ell1$ 은 GT와 Prediction 의 차이로 indiscriminate 하게 학습.</li>
      <li><a href="https://arxiv.org/pdf/1704.03915.pdf">Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution</a>에 따르면 <strong>Charbonnier loss</strong>(미분가능한 $\ell1$의 분산)가 $\ell1$, $\ell2$ 보다 성능이 뛰어남.</li>
      <li><a href="https://arxiv.org/pdf/1706.03142.pdf">Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D Electron Microscopy</a> 에 따르면 <strong>Cubic-weighted mean square error</strong> 가 Generated 영상과 Ground truth 간의 차이가 큰 “어려운” 부분의 성능을 강조.</li>
      <li>다음과 같은 Loss 제안.</li>
      <li>$\epsilon$ 은 default로 $10^{-6}$</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">L^G_{SC} = \frac{1}{rLWH}\sum_{x,y,z=1,1,1}^{L,W,rH}\sqrt{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2+\epsilon}\cdot\bigg(\frac{1}{2}+\frac{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2}{2max((I^{GT}-I^Y)^2)}\bigg)</script>

<ul>
  <li>3-D Gradient Correction Loss
    <ul>
      <li>Charbonnier Loss는 Pixelwise difference에 대한 Loss, Gradient에 대한 손실을 줄 수 있음.</li>
      <li>다음과 같이 각 axis에 대한 Gradient 를 이용하여 Loss 제안.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">L^G_{GC} = \mathbb{E}[(\nabla_{x}I^{GT}_{x,y,z} - \nabla_{x}I^Y_{x,y,z})^2] \\  + \mathbb{E}[(\nabla_{y}I^{GT}_{x,y,z} - \nabla_{y}I^Y_{x,y,z})]^2\\ + \mathbb{E}[(\nabla_{z}I^{GT}_{x,y,z} - \nabla_{z}I^Y_{x,y,z})^2]</script>

<ul>
  <li>Adversarial Loss
    <ul>
      <li>LSGAN Loss 사용.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">L^D=\frac{1}{2}\mathbb{E}[(D(I^{GT}, I^A, I^S)-1)^2+D(I^Y, I^A, I^S)^2]</script>

<script type="math/tex; mode=display">L^G_{AD}=\mathbb{E}[(D(I^Y, I^A, I^S)-1)^2]</script>

<ul>
  <li>$\ell_2$ Weight Regularization Loss
    <ul>
      <li>(Loss는 아니지만…)</li>
      <li>Overfitting을 방지하기 위해 사용.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">L^G_{WR} = \sum\Vert W_G\Vert^2_2</script>

<ul>
  <li>3D-Y-Net-GAN Loss
    <ul>
      <li>$L_G = L^G_{SC} + \lambda_1L^G_{GC} + \lambda_2L^G_{AD} + \lambda_3L^G_{WR}$</li>
    </ul>
  </li>
  <li>3D-DenseU-Net Loss
    <ul>
      <li>$L = L_{SC} + \lambda_1L_{GC} + \lambda_3L_{WR}$</li>
    </ul>
  </li>
</ul>

<h2 id="experimental-result">Experimental Result</h2>
<ul>
  <li>Multiplanar 의 효율성을 검증하기 위해 다음과 같이 세 가지 경우로 나눔.
    <ul>
      <li>1) Axial, Sagittal 둘 다 이용.</li>
      <li>2) Axial 만 이용.</li>
      <li>3) Saigittal 만 이용.</li>
    </ul>
  </li>
  <li>Loss function을 검증하기 위해 네 가지 경우로 나눔.
    <ul>
      <li>1) $\ell1norm + L_{GC} + L_{AD} + L_{WR}$ (pixelwise loss를 $\ell1norm$으로 대체.)</li>
      <li>2) $L_{SC} + L_{GC} + L_{WR}$</li>
      <li>3) $L_{SC} + L_{AD} + L_{WR}$</li>
      <li>4) $L_{SC} + L_{GC} + L_{AD} + L_{WR}$</li>
    </ul>
  </li>
  <li>Evalutaion Method 로는 아래와 같이 네 가지 기법과 자신들의 Network
    <ul>
      <li>1) <a href="https://ieeexplore.ieee.org/document/1163711">Bicubic interpolation</a></li>
      <li>2) <a href="https://ieeexplore.ieee.org/document/5466111">Sparse representation</a></li>
      <li>3) <a href="https://arxiv.org/abs/1706.03142">3D-SRU-Net</a></li>
      <li>4) 3D-Y-Net-GAN</li>
      <li>5) 3D-Y-Net-GAN + 3D-DenseU-Net</li>
    </ul>
  </li>
  <li>Metrics으로는 다음 세 가지 사용.
    <ul>
      <li>PSNR(Peak Signal-to-Noise Ratio)</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">\begin{alignedat}{2}
MAX_I = 255\\
PSNR = 20\cdot\log_{10}\Bigg(\frac{MAX_I}{\sqrt{\frac{1}{rLWH}\sum_{x, y, z}(I^R_{x,y,z}-I^{GT}_{x,y,z})^2}}\Bigg)
\end{alignedat}</script>

<ul>
  <li>SSIM(Structural SIMilarity)</li>
</ul>

<script type="math/tex; mode=display">\begin{alignedat}{2}
L : 255(\text{dynamic range})\\
\mu : \text{Variance}\\
\mu_{ab} : \text{Covariance}\\
c_1 = (k_1L)^2\\
c_2 = (k_2L)^2\\
SSIM=\frac{(2\mu_a\mu_b+c_1)(2\sigma_{ab}+c_2)}{(\mu_a^2+\mu_b^2+c_1)(\sigma_a^2+\sigma_b^2+c_2)}
\end{alignedat}</script>

<ul>
  <li>NMI(Normalized Mutual Information)</li>
</ul>

<script type="math/tex; mode=display">\begin{alignedat}{2}
H(X) = -\sum_{x_i}\in{X}p(x_i)\log{p(x_i)} \\
H(X, Y) = -\sum_{y_i\in{Y}} \sum_{x_i\in{X}}p(x_i, y_i)\log{p(x_i, y_i)}\\
NMI(X, Y) = 2\frac{H(X) + H(Y) - H(X, Y)}{H(X)+H(Y)}
\end{alignedat}</script>

<ul>
  <li>pixel 값을 [-1, 1]로 clipping -&gt; 다시 8-bit gray scale로 변환.</li>
  <li>Generated MR images 와 Ground truth가 비슷할 수록 높은 값을 가짐.</li>
</ul>

<h3 id="a-data-and-preprocessing">A. Data and Preprocessing</h3>

<ul>
  <li>총 154 samples의 2~5세 유아 Axial, Sagittal Thick MRI, Axial Thin MRI</li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/thin/tab01.PNG" /></p>

<ul>
  <li>Table 1. 과 같은 parameter 사용.</li>
  <li>Dataset 분할
    <ul>
      <li>Cross Validation Dataset : 40 samples</li>
      <li>Test 1 Dataset : 65 samples</li>
      <li>Test 2 Dataset : 49 samples</li>
    </ul>
  </li>
  <li>Preprocessing
    <ul>
      <li>각 영상별로 다른 parameter를 가지고 있고 intensities 도 다양하기 때문에 spatial misalignment, intensity imblance를 발견.</li>
      <li>Registration을 위해 SPM12 를 이용하여 unified spatial normalization 수행.
        <ul>
          <li>
            <ol>
              <li>DICOM to NIfTI</li>
            </ol>
          </li>
          <li>
            <ol>
              <li>Segment gray matter, white matter, cerebrospinal fluid, skull, scalp, and air mask.</li>
            </ol>
          </li>
          <li>
            <ol>
              <li>Nonlinear deformation field</li>
            </ol>
          </li>
          <li>
            <ol>
              <li>ICBM Asian brain template in affine regularization</li>
            </ol>
          </li>
        </ul>
      </li>
      <li>Grayscale Normalization
        <ul>
          <li>MRI 는 16 bit..</li>
          <li>단순 linear transformation 으로 [-1, 1]로 mapping.</li>
        </ul>
      </li>
      <li>Histogram Matching
        <ul>
          <li>고정된 샘플을 reference로 histogram matching 적용.</li>
          <li>histogram imbalance 제거.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Data Augmentation
    <ul>
      <li>Radial Transformation
        <ul>
          <li><strong><a href="https://arxiv.org/pdf/1708.04347.pdf">Image Augmentation using Radial Transform for Training Deep Neural Networks</a></strong></li>
        </ul>
      </li>
      <li>Mirror Reflection</li>
    </ul>
  </li>
</ul>

<h3 id="b-experimental-settings">B. Experimental Settings</h3>
<ul>
  <li>5-fold cross-validation 적용.</li>
  <li>35 개중 랜덤으로 28:7로 training:validation . <strong>-&gt; 앞에선 40개라더니..?</strong></li>
  <li>Training 3D-Y-Net-GAN
    <ul>
      <li>Batch Size : 16</li>
      <li>Epochs : 200</li>
      <li><a href="https://arxiv.org/pdf/1412.6980.pdf">Adam Optimizer Parameter</a>
        <ul>
          <li>$\beta_1$: 0.9</li>
          <li>Learning rate schedule
            <ul>
              <li>Initial value : 5*10<sup>-4</sup></li>
              <li>Decay Step : 252</li>
              <li>Decay rate : 0.989</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$\lambda_1, \lambda_2, \lambda_3$ : 0.2, 0.02, 0.1</li>
      <li>He initializer</li>
    </ul>
  </li>
  <li>Training 3D-DenseU-Net
    <ul>
      <li>Batch Size : 12</li>
      <li>Epochs : 300</li>
      <li>Adam Optimizer Parameter
        <ul>
          <li>$\beta_1$: 0.9</li>
          <li>Learning rate schedule
            <ul>
              <li>Initial value : 5*10<sup>-4</sup></li>
              <li>Decay Step : 373</li>
              <li>Decay rate : 0.989</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$\lambda_1, \lambda_3$ : 1, 0.001</li>
      <li>He initializer</li>
    </ul>
  </li>
  <li>SR Parameter
    <ul>
      <li>Dictionary size = 512</li>
      <li>Patch number = 100,000</li>
      <li>Patch size = 13 x 13</li>
      <li>Sparsity Regularization = 0.15</li>
      <li>Overlap = 12.</li>
    </ul>
  </li>
  <li>Training 3D-SRU-Net
    <ul>
      <li>Batch Size : 32</li>
      <li>Epochs : 300</li>
      <li>Adam Optimizer Parameter
        <ul>
          <li>$\beta_1$: 0.9</li>
          <li>Initial value : 5*10<sup>-4</sup></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="c-ablation-experiment-on-input-data">C. Ablation Experiment On Input Data</h4>
<ul>
  <li>Input을 변경하면서 실험 진행. 
<img src="https://jjerry-k.github.io/public/img/thin/fig06.PNG" />
<img src="https://jjerry-k.github.io/public/img/thin/tab02.PNG" /></li>
  <li>Axial 과 Sagittal 을 같이 사용했을 때가 좀 더 세부적인 구조, 적은 왜곡을 보임.
    <ul>
      <li>두 축의 영상이 서로 조합하여 reconstruction task를 향상.</li>
    </ul>
  </li>
  <li>Quantitive evaluation 에서도 더 높은 지표를 산출.</li>
</ul>

<h4 id="d-ablation-experiment-on-loss-function">D. Ablation Experiment On Loss Function</h4>
<ul>
  <li>Loss를 변경하면서 실험 진행.
<img src="https://jjerry-k.github.io/public/img/thin/fig07.PNG" />
<img src="https://jjerry-k.github.io/public/img/thin/tab03.PNG" /></li>
  <li>Self-Adaptive Charbonnier Loss에 비해 $\ell1$ norm 이 흐린 영상을 생성.</li>
  <li>Without Gradient Correction Loss
    <ul>
      <li>덜 선명한 영상을 생성.</li>
    </ul>
  </li>
  <li>Without Adversarial Loss
    <ul>
      <li>덜 realistic 영상을 생성. <strong>-&gt; ?????그냥 쓴 말인가..</strong></li>
    </ul>
  </li>
  <li><strong>Table3 …지표 좀 이상..</strong></li>
</ul>

<h4 id="e-comparison-with-other--methods">E. Comparison With Other  Methods</h4>
<ul>
  <li>다른 Method들과 비교.
<img src="https://jjerry-k.github.io/public/img/thin/fig08.PNG" />
<img src="https://jjerry-k.github.io/public/img/thin/tab04.PNG" /></li>
  <li>제안한 method로 생성된 image가 가장 Realistic하고 Ground truth 와 가장 비슷하다고 함.</li>
  <li>대부분 Quantitative evaluation 에서 제안한 method가 다른 것들을 다 뛰어넘음.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>
<ul>
  <li><strong>제안한 Method 에선 Data preprocessing이 매우 중요하다……</strong></li>
</ul>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2019/07/05/Thin/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2019/07/05/Thin/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page3">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/">Newer</a>
    
  
</div>


        </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <!-- Modal -->
  <div class="modal fade" id="myModal" role="dialog">
    <div class="modal-dialog">

      <!-- Modal content-->
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal">&times;</button>
          <h4 class="modal-title">Search</h4>
        </div>
        <div class="modal-body">

    <h4><div id="search-container">
      <input type="text" id="search-input" placeholder="검색어 입력" class="input" style="font-size: 1rem; width: 100%; padding: 10px; border: 0px; outline: none; float: left; background: #cecece;" autofocus>
    </div></h4><br>

      <div id="results">
        <h1></h1>
      <ul class="results"></ul>
      </div>
      <br><ul id="results-container"></ul>
    </div>

<!-- Script pointing to jekyll-search.js -->


<script type="text/javascript">
      SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('results-container'),
        json: '/dest/search.json',
        searchResultTemplate: '<h4 style="margin-bottom:-0.5rem;"><a class="post-title" style="color:#ac4142;" href="{url}" title="{desc}">{title}</a> <small>{category}</small></h4>',
        noResultsText: '<h4><br>문서가 존재하지 않습니다.</h4>',
        limit: 15,
        fuzzy: false,
        exclude: ['Welcome']
      })
</script>
        </div>
      </div>

    </div>
  </div>

</div>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');
        document.addEventListener('click', function(e) {
          var target = e.target;
          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>

    
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-126636477-1', 'auto');
        ga('send', 'pageview');
      </script>
    

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (absbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-1054696837285307",
          enable_page_level_ads: true
      });
    </script>
  </body>
  
  <script id="dsq-count-scr" src="//jerry.disqus.com/count.js" async></script>
  
</html>
