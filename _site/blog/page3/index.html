<!DOCTYPE html>
<html lang="en-us">
  <head>
  <head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
    <!-- Enable responsiveness on mobile devices-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
    <title>
      
        Blog &middot; Jerry's Blog
      
    </title>
  
    <!-- CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
    <link rel="stylesheet" href="/public/css/main.css">
  
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
    <link rel="shortcut icon" href="/public/favicon.ico">
  
    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  
    <!-- scroll -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script>
      $( window ).scroll( function() {
        if ( $( this ).scrollTop() > 500 ) {
          $( '.top' ).fadeIn();
        } else {
          $( '.top' ).fadeOut();
        }
      } );
      $( '.top' ).click( function() {
        $( 'html, body' ).stop().animate( { scrollTop : 0 }, 100);
        return false;
      } );
    </script>
    
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (absbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-1054696837285307",
          enable_page_level_ads: true
      });
    </script>
    
    <!-- 
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        //jax: ["input/TeX", "output/HTML-CSS"],
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$', '$'] ],
          displayMath: [ ['$$', '$$'] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
        //,
        //displayAlign: "left",
        //displayIndent: "2em"
      });
      MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
      });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">
    </script>
     -->
    
    
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      //jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
  

    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="/dest/simple-jekyll-search.js" type="text/javascript"></script>
    <script type="text/javascript">
    $(document).ready(function(){
      document.search.searchinput.focus();
    });
    </script>
  </head>
  
  <style>blockquote {font-size: 1em; line-height: 1.4}</style>
  
  <!-- New line Start-->
  <!--  -->
  <!-- New line End -->
  </head>
  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="https://gravatar.com/22a6447c0c965de55f4ce9691aed2af4">
          <img src="https://www.gravatar.com/avatar/22a6447c0c965de55f4ce9691aed2af4?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p></p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me  :  
        
        
        
        <a href="https://github.com/jjerry-k">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="https://www.facebook.com/jerry.kim.566">
          <i class="fa fa-facebook" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:jaeyeol2931@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Contents
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/blog/">
                Blog
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/python/">
                Python
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/ubuntu/">
                Ubuntu
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/living/">
                Living
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/deeplearning/">
                Deep Learning
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 Jerry. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
          <div class="top" style="position:fixed; right:10%; bottom:15px; display:none; z-index:9999;">
            <a href="#">
              <img src="https://jjerry-k.github.io/public/top.png" width="40" height="40" class="top" style="border-radius:5px; background-color: #000; margin-bottom:0; margin-right:7px; float:left;">
            </a>
            
            <a href="/blog//">
              <img src="https://jjerry-k.github.io/public/contents.png" width="40" height="40" class="top" style="border-radius:5px; background-color: #000; margin-bottom:0; float:left;">
            </a>
          </div>
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title" align="center">
            <a href="/" title="Home" title="Jerry's Blog">
              <img class="masthead-logo" src="/public/logo.png"/>
            </a>
            <small></small>
            <a href="https://donaricano.com/mypage/1391188679_guPzXC" target="_blank">
              <img src="https://d1u4yishnma8v5.cloudfront.net/mobile-gift.png" width="60" height="60" style="position:absolute; top:0.25rem; right:4rem" alt="donaricano-btn" style="height: 130px !important;width: 130px !important;" />
            </a>
            <img src="http://jjerry-k.github.io/public/search.png" width="20" height="20" style="position:absolute; top:1.3rem; right:1.5rem" data-toggle="modal" data-target="#myModal">
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/06/28/nipa_docker/">
        NIPA x Docker !
      </a>
    </h1>

    <span class="post-date">28 Jun 2020</span>
     | 
    
    <a href="/blog/tags/#tools" class="post-tag">Tools</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <blockquote>
  <p>원래 NIPA GPU 서버를 대여받은 후에 포트 포워딩을 먼저 해줘야 합니다.
하지만 그 부분에 대해선 보안적인 부분이 있기 때문에 생략하겠습니다.</p>
</blockquote>

<p>이번엔 NIPA 내 개인 환경 세팅에 대해 포스팅을 해보려 합니다.</p>

<p>개인마다 원하는 환경이 다르기 때문에 정말 필요하죠.</p>

<p>물론 기본적으로 설치된 Anaconda  환경으로도 충분할 수 있지만 살짝쿵 문제가 있습니다.</p>

<p>문제에 대해 살펴보겠습니다.</p>

<p>NIPA GPU 서버에 접속을 하면 다음과 같은 화면이 출력됩니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled.png" alt="Untitled.png" /></p>

<p>먼저 말씀드렸던 conda 환경으로 기본적으로 다양한 환경이 제공되네요.</p>

<p>저의 경우 이번에 tf-nightly가 필요했습니다.</p>

<p>그래서 TensorFlow2, python3.6 환경을 activate  한 후 설치를 시도했죠.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_1.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%201.png" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">what…?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_2.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%202.png" /></td>
    </tr>
  </tbody>
</table>

<p>음….conda 버전의 문제인가 싶어서 base conda를 update  하려 했습니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_3.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%203.png" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">what…?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_2.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%202.png" /></td>
    </tr>
  </tbody>
</table>

<p>뭐야..이건 또 왜 안되는거야… 짜증이 났습니다.</p>

<p>대충 <code class="language-plaintext highlighter-rouge">너무 옛날 버전의 conda니까 최소 4.8로 재설치 해주세요.</code> 라는 내용입니다.</p>

<p><code class="language-plaintext highlighter-rouge">하....이건 좀 너무한데...그냥 Docker나 설치하자..</code> 라는 생각을 하게 되었습니다.</p>

<p>그럼 Docker 설치에 대해 포스팅 해보겠습니다.</p>

<p>Docker 에 대한 자세한 설명은 하지 않을 겁니다.</p>

<p>홈페이지 혹은 Docker 에 대한 포스팅을 참고하시기 바랍니다.</p>

<p>간단히 말씀드리면 OS 단계까지 가상환경을 만드는 겁니다.</p>

<p>그럼 설치 방법에 대해 적겠습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SET UP THE REPOSITORY</span>
<span class="nb">sudo </span>apt-get update

<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="se">\</span>
    apt-transport-https <span class="se">\</span>
    ca-certificates <span class="se">\</span>
    curl <span class="se">\</span>
    gnupg-agent <span class="se">\</span>
    software-properties-common

curl <span class="nt">-fsSL</span> https://download.docker.com/linux/ubuntu/gpg | <span class="nb">sudo </span>apt-key add -

<span class="nb">sudo </span>apt-key fingerprint 0EBFCD88

<span class="nb">sudo </span>add-apt-repository <span class="se">\</span>
   <span class="s2">"deb [arch=amd64] https://download.docker.com/linux/ubuntu </span><span class="se">\</span><span class="s2">
   </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2"> </span><span class="se">\</span><span class="s2">
   stable"</span>

<span class="c"># INSTALL DOCKER ENGINE</span>
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>docker-ce docker-ce-cli containerd.io

<span class="c"># VERIFY THAT DOCKER ENGINE IS INSTALLED CORRECTLY</span>
<span class="nb">sudo </span>docker run hello-world
</code></pre></div></div>

<p>만약에 제대로 설치 되었다면 마지막에 다음과 같은 출력이 남습니다</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_4.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%204.png" /></p>

<p>여기까지 하시면 기본 Docker 설치는 끝났습니다.</p>

<p>하지만 이것만 설치하면 GPU 는 사용하지 못합니다.</p>

<p>GPU를 쓰기 위해선 <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>를 설치를 해야 합니다.</p>

<p>nvidia-docker는 간단히 말하면 docker 에서 데스크탑의 GPU를 사용할 수 있도록 nvidia에서 만든(?)것입니다.</p>

<p>설치법은 다음과 같습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ubuntu 16.04/18.04/20.04, Debian Jessie/Stretch/Buster</span>
<span class="c"># Add the package repositories</span>
<span class="nv">distribution</span><span class="o">=</span><span class="si">$(</span><span class="nb">.</span> /etc/os-release<span class="p">;</span><span class="nb">echo</span> <span class="nv">$ID$VERSION_ID</span><span class="si">)</span>
curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/nvidia-docker/gpgkey | <span class="nb">sudo </span>apt-key add -
curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/nvidia-docker/<span class="nv">$distribution</span>/nvidia-docker.list | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/nvidia-docker.list

<span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> nvidia-container-toolkit
<span class="nb">sudo </span>systemctl restart docker

<span class="c"># Test nvidia-docker</span>
docker run <span class="nt">--gpus</span> all nvidia/cuda:10.0-base nvidia-smi
</code></pre></div></div>

<p>이 또한 설치가 제대로 되었다면 마지막에 다음과 같이 <code class="language-plaintext highlighter-rouge">nvidia-smi</code> 출력이 나올 겁니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_5.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%205.png" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">하….편안….</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://jjerry-k.github.io/public/img/nipa_docker/Untitled_6.png" alt="NIPA%20x%20Docker%20ef94d24dfbc64a6cae24a83a59bd352f/Untitled%206.png" /></td>
    </tr>
  </tbody>
</table>

<p>이번엔 NIPA에 Docker 설치하는 과정을 포스팅 해봤습니다.</p>

<p>공짜로 빌려주는 건 좋으나 환경 구축은 역시나….해야 하네요.</p>

<p>제가 쓰는 Docker image는 <a href="https://jjerry-k.github.io/living/2020/05/05/dockerfile/">개인적인 도커 파일</a> 에 있으니 참고하세요!</p>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/06/28/nipa_docker/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/06/28/nipa_docker/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/06/27/nipa_apply/">
        NIPA 컴퓨팅 자원 신청 방법 !
      </a>
    </h1>

    <span class="post-date">27 Jun 2020</span>
     | 
    
    <a href="/blog/tags/#tools" class="post-tag">Tools</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p><a href="https://jjerry-k.github.io/deeplearning/2020/06/26/nipa_intro/">저번 포스팅</a>에 이어 이번엔 NIPA 컴퓨팅 자원 이용 신청에 관한 포스팅을 해보려 합니다!</p>

<p>내용 출처: <a href="http://www.aihub.or.kr/node/254">http://www.aihub.or.kr/node/254</a></p>

<h3 id="이용-절차">이용 절차</h3>

<ul>
  <li>일반 사용자
    <ul>
      <li>신청 대상: AI 제품·서비스를 연구·개발하고자 하는 국내 중소·벤처 기업, 스타트업, 공공기관, 대학교(원), 일반 협·단체</li>
      <li>신청 기간: 2020. 1. 6.(월) ∼ 2. 7.(금)※ 추가 모집 : 2020.5.25.(월) ~ 11.30.(월)</li>
      <li>신청 절차: 이용 신청서 작성 → 온라인 신청 사이트에 제출 → 서면 심사 → 선정결과 발표<br />
  ※ 선정기준 : 지원 자격요건, AI 연구‧개발대상 여부, 사용계획서(목적, 연구‧개발분야, 사용내용, 필요성 등)를 중심으로 심사‧평가<br />
  ※ 모집 규모 대비 초과 신청 시 중소·벤처기업, 공공기관, 대학교(원)에 자원이 우선 할당될 수 있음</li>
    </ul>
  </li>
  <li>수시 사용자
    <ul>
      <li>신청 대상: 일반 사용자와 개발자, 학생 등 개인</li>
      <li>신청 기간: 2020.4.10.(금) ~ 2020.12.21.(월)</li>
      <li>신청 절차: 이용신청 작성‧제출 → AI 연구‧개발대상 여부 확인 → 여유 자원 확인 → 서비스 이용</li>
    </ul>
  </li>
</ul>

<p>각각의 신청서 양식은 다음과 같습니다.</p>

<h3 id="일반-사용자용-신청서">일반 사용자용 신청서</h3>

<ul>
  <li><a href="http://www.aihub.or.kr/sites/default/files/2020-05/[일반">다운로드 링크</a></li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/nipa_apply/Untitled.png" alt="Untitled.png" /></p>

<h3 id="수시-사용자용-신청서">수시 사용자용 신청서</h3>

<ul>
  <li><a href="http://www.aihub.or.kr/sites/default/files/2020-05/[%EC%88%98%EC%8B%9C%20%EC%82%AC%EC%9A%A9%EC%9E%90%EC%9A%A9]%20%EA%B3%A0%EC%84%B1%EB%8A%A5%20%EC%BB%B4%ED%93%A8%ED%8C%85%20%EC%9E%90%EC%9B%90%20%EC%9D%B4%EC%9A%A9%20%EC%8B%A0%EC%B2%AD%EC%84%9C.hwp">다운로드 링크</a></li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/nipa_apply/Untitled_1.png" alt="Untitled_1.png" /></p>

<p>당연한거지만 일반 사용자가 수시 사용자에 비해 작성해야할 것이 많습니다.</p>

<p>신청서 작성 방법은 예시가 너무 잘 적혀있기 때문에 따로 설명을 드리지 않겠습니다. (궁금하신게 있다면 따로 댓글 남겨주세요!)</p>

<p>신청서를 작성하신 후엔 AIHub 사이트 로그인을 하신 후 <code class="language-plaintext highlighter-rouge">이용신청</code> 으로 넘어갑니다.<br />
그럼 다음과 같은 화면이 나오는데 (수시 사용자 기준) <strong>1~8번 중 해당하는 부분 선택하</strong>고 <strong>개인 정보</strong> 채워주시면 됩니다!</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_apply/Untitled_2.png" alt="Untitled_2.png" /></p>

<p>다 채운 후 이용신청을 누르시면 한…. 하루, 이틀? 정도 후에 메일과 핸드폰으로 문자 안내가 옵니다!<br />
메일에는 NIPA 서버 사용방법, 문자에는 관리 페이지 이용 안내가 옵니다. <br />
신청이 어렵지 않으니 많이 많이 신청하시고 즐거운 딥러닝 공부, 연구하세요! (홍보 아님)</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_apply/go.png" alt="go.png" /></p>

<h2 id="ps">P.S.</h2>
<ul>
  <li>중요! ! ! <strong>수시 사용자는 계속 사용하려면 10일마다 연장 신청 해야함. 꼭! 반드시!</strong></li>
  <li><strong>위 내용을 지키지 않으면 서버 초기화되서 내용들 다 사라짐!!!</strong></li>
  <li>너무 대충 포스팅을…하는걸까…</li>
</ul>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/06/27/nipa_apply/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/06/27/nipa_apply/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/06/26/nipa_intro/">
        NIPA...NIPA가 뭐죠..
      </a>
    </h1>

    <span class="post-date">26 Jun 2020</span>
     | 
    
    <a href="/blog/tags/#tools" class="post-tag">Tools</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p>요즘 커뮤니티(V-AIS) 에선 NIPA에 대한 얘기가 많이 나옵니다.<br />
NIPA는 <a href="https://www.nipa.kr/"><strong>정보통신산업진흥원</strong></a> 의 약자인데 <code class="language-plaintext highlighter-rouge">근데 그래서 이게 왜..?</code> 라는 의문을 가지실 겁니다. <br />
지금 AI Hub에서 지원하는 <a href="http://www.aihub.or.kr/node/223"><strong>AI 컴퓨팅 자원 지원 사업</strong></a>을 하고 있는데요.<br />
그 사업에서 컴퓨팅 자원을 관리하는게 NIPA 입니다. 그래서 NIPA, NIPA 하죠..</p>

<p>간단하게 그림으로 보여드리면 이렇습니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_intro/Untitled.png" alt="Untitled.png" /></p>

<p>더 간단하게 설명드리면 <strong><code class="language-plaintext highlighter-rouge">GPU서버 지원해드림 ㅇㅇ</code></strong> 이겁니다.</p>

<p>어느 정도 사양이냐…</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_intro/Untitled_1.png" alt="Untitled_1.png" /></p>

<p><strong>일반 사용자</strong>와 <strong>수시 사용자</strong>로 나뉘는데요. 그 차이는 사이트에서 확인하시길..</p>

<p>이렇게 보면 GPU로 뭐 쓰는지 잘 모르시겠죠. 그래서 가져왔습니다.</p>

<p>제가 포스팅, 환경구축과 같은 자료를 만들기 위해 신청한 서버입니다.</p>

<p><img src="https://jjerry-k.github.io/public/img/nipa_intro/Untitled_2.png" alt="Untitled_2.png" /></p>

<p>위 GPU 사진은 <strong>수시 사용자</strong> 기준입니다. 일반 사용자면 V100 두개겠네요.</p>

<p>뭐 이런 서버를 <strong>공짜</strong>로 제공을 해줍니다.</p>

<p>집에 서버가 따로 없는 분들이라면 이보다 좋은 건 없겠죠. (게다가 VRAM이 32기가임….)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>헤헿 겁나 좋군</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://jjerry-k.github.io/public/img/nipa_intro/good.png" alt="good.png" /></td>
    </tr>
  </tbody>
</table>

<p>다음엔 NIPA 신청 방법에 대한 포스팅은 간단히 해보겠습니다.</p>

<p>그럼 모두 즐거운 딥러닝 공부, 연구하세요.</p>

<h2 id="ps">P.S.</h2>
<ul>
  <li>수시 사용자의 경우 기본 10일 사용, 10일마다 사용 연장신청을 해야함.</li>
  <li>당연하지만 신청해놓고 사용안하면 검열 후 강제 반납.</li>
</ul>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/06/26/nipa_intro/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/06/26/nipa_intro/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/06/14/tf2_multi_gpu/">
        TensorFlow Multi GPU 사용법
      </a>
    </h1>

    <span class="post-date">14 Jun 2020</span>
     | 
    
    <a href="/blog/tags/#tensorflow" class="post-tag">TensorFlow</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <p>이번 포스팅은 Multi GPU 시스템에서 Google의 머신러닝 오픈 소스 플랫폼인 <a href="https://www.tensorflow.org/">TensorFlow</a>사용법에 관한 것입니다!<br />
거두절미하고 바로 코딩으로 들어가겠습니다!</p>

<h2 id="single-gpu-예시">Single GPU 예시</h2>
<ul>
  <li>다음 코드는 Single GPU를 이용하여 mnist data를 분류하는 코드입니다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import Package
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Data Prepare
</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Train Data's Shape : "</span><span class="p">,</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test Data's Shape : "</span><span class="p">,</span> <span class="n">test_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Build Network
</span><span class="n">cnn</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">,)))</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="n">cnn</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span> <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>                
<span class="k">print</span><span class="p">(</span><span class="s">"Network Built!"</span><span class="p">)</span>

<span class="c1"># Training Network
</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="multi-gpu-예시">Multi GPU 예시</h2>
<ul>
  <li>다음 코드는 Multi GPU를 이용한 코드입니다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import Package
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Data Prepare
</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Train Data's Shape : "</span><span class="p">,</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test Data's Shape : "</span><span class="p">,</span> <span class="n">test_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Build Network
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
<span class="k">with</span> <span class="n">strategy</span><span class="p">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">cnn</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">,)))</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

    <span class="n">cnn</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span> <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>                
<span class="k">print</span><span class="p">(</span><span class="s">"Network Built!"</span><span class="p">)</span>

<span class="c1"># Training Network
</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="n">batch_size_each_gpu</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_each_gpu</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</code></pre></div></div>

<p>어렵지 않습니다. <code class="language-plaintext highlighter-rouge">Build Network</code> 주석 부분과 <code class="language-plaintext highlighter-rouge">Training Network</code> 부분에 <code class="language-plaintext highlighter-rouge">batch_size</code>만 조금 수정해주시면 끝납니다!<br />
<img src="https://jjerry-k.github.io/public/img/tf_multi_gpu/bob.png" alt="img" /></p>

<p>하지만 이렇게 하면 무식하게 GPU의 모든 메모리를 할당합니다.<br />
그렇기 떄문에 다음과 같이 코드를 추가하여 <code class="language-plaintext highlighter-rouge">필요한 만큼</code> 할당하도록 합니다.</p>

<h2 id="필요한-만큼의-gpu-메모리만-사용하기">필요한 만큼의 GPU 메모리만 사용하기</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import Package
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Data Prepare
</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Train Data's Shape : "</span><span class="p">,</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test Data's Shape : "</span><span class="p">,</span> <span class="n">test_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># For Efficiency
</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s">"Physical GPUs,"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s">"Logical GPUs"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="c1"># Build Network
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
<span class="k">with</span> <span class="n">strategy</span><span class="p">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">cnn</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">,)))</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">cnn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

    <span class="n">cnn</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span> <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>                
<span class="k">print</span><span class="p">(</span><span class="s">"Network Built!"</span><span class="p">)</span>

<span class="c1"># Training Network
</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="n">batch_size_each_gpu</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_each_gpu</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</code></pre></div></div>

<p>기존 Multi GPU 코드와 달라진 점은</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">For Efficiency</code>라는 부분이 추가.</li>
  <li><code class="language-plaintext highlighter-rouge">strategy = tf.distribute.MirroredStrategy()</code>을 <code class="language-plaintext highlighter-rouge">Build Network</code>에서 <code class="language-plaintext highlighter-rouge">For Efficiency</code>의 가장 첫번째 라인으로 이동.</li>
</ol>

<p>이렇게 변경 후 실행 후 nvidia-smi와 같은 모니터링 툴을 확인해보시면 이전과는 다르게 GPU 메모리를 필요한 만큼만 사용하는걸 보실 수 있습니다!</p>

<h2 id="ps">P.S</h2>
<ul>
  <li>다음은 뭘로 포스팅하지…</li>
</ul>

<h2 id="번외편-using-gradient-tape">번외편 (Using gradient tape)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># %%
# Import Package
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">utils</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Currently, memory growth needs to be the same across GPUs
</span>        <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s">"Physical GPUs,"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s">"Logical GPUs"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># Memory growth must be set before GPUs have been initialized
</span>        <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="c1"># %%
# Data Prepare
</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="n">batch_size_each_gpu</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_each_gpu</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">)</span>

<span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_x</span><span class="o">/</span><span class="mf">255.</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Train Data's Shape : "</span><span class="p">,</span> <span class="n">train_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test Data's Shape : "</span><span class="p">,</span> <span class="n">test_x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># %%
# Build Network
</span><span class="k">class</span> <span class="nc">build_model</span><span class="p">(</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">build_model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Network Built!"</span><span class="p">)</span>

<span class="c1"># Set mirrored Strategy
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="k">with</span> <span class="n">strategy</span><span class="p">.</span><span class="n">scope</span><span class="p">():</span>
    
    <span class="c1"># Prepare dataset 
</span>    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)).</span><span class="n">shuffle</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)).</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> 
    <span class="n">train_dist_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="p">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
    
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)).</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> 
    <span class="n">test_dist_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="p">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>

    <span class="c1"># Make Network
</span>    <span class="n">cnn</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>

    <span class="c1"># Set Loss &amp; Metric function
</span>    <span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">Reduction</span><span class="p">.</span><span class="n">NONE</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
        <span class="n">per_example_loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">per_example_loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'test_loss'</span><span class="p">)</span>

    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'train_accuracy'</span><span class="p">)</span>
    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'test_accuracy'</span><span class="p">)</span>

    <span class="c1"># Set optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">()</span>

    <span class="c1"># Define taining, test function
</span>    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">cnn</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">cnn</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>

        <span class="n">train_accuracy</span><span class="p">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span> 
    
    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">t_loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

        <span class="n">test_loss</span><span class="p">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">t_loss</span><span class="p">)</span>
        <span class="n">test_accuracy</span><span class="p">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="c1"># Define training, test function suitable for Mirrored Strategy 
</span>    <span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">distributed_train_step</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">):</span>
        <span class="n">per_replica_losses</span> <span class="o">=</span> <span class="n">strategy</span><span class="p">.</span><span class="n">experimental_run_v2</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">strategy</span><span class="p">.</span><span class="nb">reduce</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">per_replica_losses</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
 
    <span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">distributed_test_step</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">strategy</span><span class="p">.</span><span class="n">experimental_run_v2</span><span class="p">(</span><span class="n">test_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset_inputs</span><span class="p">,))</span>

    <span class="c1"># Train Network
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    
        <span class="c1"># Training Loop
</span>        <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train_dist_dataset</span><span class="p">:</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">distributed_train_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>

        <span class="c1"># Test Loop
</span>        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">test_dist_dataset</span><span class="p">:</span>
            <span class="n">distributed_test_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">template</span> <span class="o">=</span> <span class="p">(</span><span class="s">"에포크 {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}"</span><span class="p">)</span>
        <span class="k">print</span> <span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">.</span><span class="n">result</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">test_accuracy</span><span class="p">.</span><span class="n">result</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

        <span class="n">test_loss</span><span class="p">.</span><span class="n">reset_states</span><span class="p">()</span>
        <span class="n">train_accuracy</span><span class="p">.</span><span class="n">reset_states</span><span class="p">()</span>
        <span class="n">test_accuracy</span><span class="p">.</span><span class="n">reset_states</span><span class="p">()</span>
</code></pre></div></div>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/06/14/tf2_multi_gpu/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/06/14/tf2_multi_gpu/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deeplearning/2020/06/13/DETR/">
        Review: DETR
      </a>
    </h1>

    <span class="post-date">13 Jun 2020</span>
     | 
    
    <a href="/blog/tags/#paper" class="post-tag">Paper</a>
    
    

    <h4 style="font-weight: 400; line-height: 1.8;"><article>
      <h1 id="end-to-end-object-detection-with-transformers">End-to-End Object Detection with Transformers</h1>

<p>Author: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko
Date: May 27, 2020
URL: https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers</p>

<h1 id="introduction">Introduction</h1>

<ul>
  <li>현재 Object detection model들은 Input부터 Output (bounding bos, category label) 까지 Direct 하지 못함.  Post processing 이 영향을 끼치기 때문에…</li>
  <li>본 논문에선 Direct prediction approach 제안.</li>
  <li>이전에도 몇몇 실험이 있었으나 그 당시에는 prior knowledge를 준다거나 성능이 별로 좋지 못했음.</li>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer</a> 를 사용.</li>
  <li>새로운 Loss function 도입.</li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled.png" alt="jjerry-k.github.io/public/img/DETR/Untitled.png" /></p>

<h1 id="related-works">Related Works</h1>

<h2 id="set-prediction">Set Prediction</h2>

<ul>
  <li>현재까지 Direct로 set(box, class)을 prediction 하는 방법이 없음.</li>
  <li>Post processing 이 없는 model 제안.</li>
  <li>이를 위해 Hungarian algorithm 기반의 loss 설계.</li>
</ul>

<h2 id="transformers-and-parallel-decoding">Transformers and Parallel Decoding</h2>

<ul>
  <li>다른 RNN 계열보다 Long sequence 에 적합한 model</li>
  <li>auto-regressive model</li>
</ul>

<h2 id="object-detection">Object detection</h2>

<h3 id="set-based-loss">Set-based loss</h3>

<ul>
  <li>기존에 bipatite matching loss 를 사용했지만  NMS 를 사용해야 성능이 향상되었음.</li>
  <li>그 후 Learnable NMS 를 사용한 방법이 제시 되었으나 hand-crafted context feature 를 사용 하기에 효율적이지 못함.</li>
</ul>

<h3 id="recurrent-detectors">Recurrent detectors</h3>

<ul>
  <li>이름에서 알 수 있듯 RNN 계열을 도입한 Object detection</li>
  <li>기존 방법에선 Small dataset을 이용했고 RNN 계열을 이용했기에 parallel 구조를 가져가지 못했음.</li>
</ul>

<h1 id="the-detr-model">The DETR model</h1>

<ul>
  <li>DETR은 크게 두 개의 장점이 있음. → a set prediction loss, a architecture</li>
</ul>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled_1.png" alt="jjerry-k.github.io/public/img/DETR/Untitled_1.png" /></p>

<h2 id="object-detection-set-prediction-loss">Object detection set prediction loss</h2>

<p>[\hat{\sigma} = {argmin}<em>{\sigma \in \mathfrak{S}_N} \sum^N_i \mathcal{L}</em>{match}(y_i, \hat{y}_{\sigma(i)})]</p>

<p>[\mathcal{L}<em>{match}(y_i, \hat{y}</em>{\sigma(i)}) = -1<em>{c_i \neq \phi}\hat{p}</em>{\sigma(i)}(c_i) +1<em>{c_i \neq \phi}\mathcal{L}</em>{box}(b_i, \hat{b}_{\sigma(i)})]</p>

<p>[\mathcal{L}<em>{Hungarian}(y, \hat{y}) = \sum^N_i[-\log\hat{p}</em>{\hat{\sigma}(i)}(c_i) +1<em>{c_i \neq \phi}\mathcal{L}</em>{box}(b_i, \hat{b}_{\hat{\sigma}(i)})]]</p>

<h3 id="bounding-box-loss">Bounding box loss</h3>

<p>[\lambda_{iou}\mathcal{L}<em>{iou}(b_i, \hat{b}</em>{\sigma(i)}) + \lambda_{\mathrm{L}1}|b_i - \hat{b}_{\sigma(i)}|_1]</p>

<h2 id="detr-architecture">DETR architecture</h2>

<h3 id="backbone">Backbone</h3>

<ul>
  <li>일반적인 Backbone 사용.</li>
  <li>마지막 feataure map은 원본 사이즈 H, W 에 비해 32분의 1 downsampling, C는 2048</li>
</ul>

<h3 id="transformer-encoder">Transformer encoder</h3>

<ul>
  <li>Attention is all you need의 Transformer의 encoder와 동일한 구조.</li>
  <li>Fixed positional encodings 으로 인하여 permutation-invariant 한 구조!</li>
</ul>

<h3 id="transformer-decoder">Transformer decoder</h3>

<ul>
  <li>Attention is all you need의 Transformer의 decoder와 동일한 구조.</li>
  <li>기존 Transformer와 차이는</li>
</ul>

<h3 id="prediction-feed-forward-networks-ffns">Prediction feed-forward networks (FFNs)</h3>

<ul>
  <li>ReLU를 사용하는 d dimension의 linear layer 3 개 사용.</li>
  <li>한 branch 에서는 Normalized center coordinate, height, width 를 예측.</li>
  <li>다른 하나의 branch는 class label을 softmax를 이용하여 예측.</li>
  <li>DETR은 항상 N개의 box에 대해 예측. 하지만 실제 object 수가 적을때는 나머지 box들을 no object 로 처리.</li>
</ul>

<h3 id="auxiliary-decoding-losses">Auxiliary decoding losses</h3>

<ul>
  <li>Transformer decoder에  Auxiliary loss 를 추가.</li>
</ul>

<h1 id="experiment">Experiment</h1>

<h2 id="comparison-with-faster-r-cnn">Comparison with Faster R-CNN</h2>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled_2.png" alt="jjerry-k.github.io/public/img/DETR/Untitled_2.png" /></p>

<h2 id="ablations">Ablations</h2>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled_3.png" alt="jjerry-k.github.io/public/img/DETR/Untitled_3.png" /></p>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled_4.png" alt="jjerry-k.github.io/public/img/DETR/Untitled_4.png" /></p>

<h2 id="panoptic-segmentation">Panoptic segmentation</h2>

<p><img src="https://jjerry-k.github.io/public/img/DETR/Untitled_5.png" alt="jjerry-k.github.io/public/img/DETR/Untitled_5.png" /></p>

<h2 id="ps">P.S</h2>
<ul>
  <li>내용 보충 예정.</li>
  <li>신박한 컨셉.</li>
</ul>

    <article></h4>
    <div class="post-more">
      
      <a href="/deeplearning/2020/06/13/DETR/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deeplearning/2020/06/13/DETR/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page2">Newer</a>
    
  
</div>


        </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <!-- Modal -->
  <div class="modal fade" id="myModal" role="dialog">
    <div class="modal-dialog">

      <!-- Modal content-->
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal">&times;</button>
          <h4 class="modal-title">Search</h4>
        </div>
        <div class="modal-body">

    <h4><div id="search-container">
      <input type="text" id="search-input" placeholder="검색어 입력" class="input" style="font-size: 1rem; width: 100%; padding: 10px; border: 0px; outline: none; float: left; background: #cecece;" autofocus>
    </div></h4><br>

      <div id="results">
        <h1></h1>
      <ul class="results"></ul>
      </div>
      <br><ul id="results-container"></ul>
    </div>

<!-- Script pointing to jekyll-search.js -->


<script type="text/javascript">
      SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('results-container'),
        json: '/dest/search.json',
        searchResultTemplate: '<h4 style="margin-bottom:-0.5rem;"><a class="post-title" style="color:#ac4142;" href="{url}" title="{desc}">{title}</a> <small>{category}</small></h4>',
        noResultsText: '<h4><br>문서가 존재하지 않습니다.</h4>',
        limit: 15,
        fuzzy: false,
        exclude: ['Welcome']
      })
</script>
        </div>
      </div>

    </div>
  </div>

</div>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');
        document.addEventListener('click', function(e) {
          var target = e.target;
          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>

    
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-126636477-1', 'auto');
        ga('send', 'pageview');
      </script>
    

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (absbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-1054696837285307",
          enable_page_level_ads: true
      });
    </script>
  </body>
  
  <script id="dsq-count-scr" src="//jerry.disqus.com/count.js" async></script>
  
</html>
